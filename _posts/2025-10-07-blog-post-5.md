---
title: 'You are on the Red Team!'
date: 2025-10-07
permalink: /posts/2025/10/blog-post-5/
tags:
  - case study
  - hallucinations
  - ethics
---

Summary
---
This Blog covers my ethical standpoint of red-teaming discussed in to the Databite Video 161 found [here](https://youtu.be/MMDw_xVeCzI?si=1O4e93qHD1lfMEVJ). This video discusses the practice of red-teaming which are practices to recognize, diagnose, and manage harmful flaws in AI and other models. The video talks about why is red-teaming important at a human level. 

Who What Why How? 
---
The guy in charge here who had a lot of energy is Borhane, the affiliate with Data & Society working with AI. The guests are Briana, Lama, Camille, and Tarleton. All of who all have a background in technology and some form of machine model moderation. 

A key idea from this amazing video that I definitely did not nod off to, is the value and importance of red-teaming, and the importance of why we need more of it. An example discussed in the video, if students are using AI in a lab and the AI outputs a solution that leads to a bad chemical reaction since all AI is, is but a probability machine, that is a real-world problem with real consequences.

This is super important now because there are many models popping up now for public use. LLMs are the biggest public thing people are using and trusting on for advice. And hullucinations are everywhere in these models from bias, to stereotyping, to dangerous outputs. The reasons for why the human aspect in red-teaming can be better xplained with an example: There is a model that ouputs an offensive claim, then a year later a different model in a different country ouputs the same offensive claim. Should the consequences be the same to who designed the model? This can be so nuanced and the reasons to why they ouput what they output can vary. 

The people in this talk support their claims through storytelling, mixing personal experiences with evidence from studies, data, and charts. They don’t just throw out opinions, they back up their ideas with real-world research and examples that make their points stick. You can tell they’ve spent years working in their fields, because they speak with the kind of confidence and clarity that only comes from deep experience. Their ability to blend facts with lived stories makes the conversation feel both credible and human.


My Take
---
I chose to listen to the Databite Video for two reasons: 1) It is there second-most recent video, so it isn't outdated.2) I'm and very interested in the ways we fight back against hullucinations given by models. I found it very perplexing that many of use are aware of the risks and the bias, but we don't have very many people who are red-teaming and imporiving it. When it comes to getting a model out there in the real world by a deadline, often the thing overlooked is the material the is "sometimes" faulty and a "good-enough" attitude is put forward to these models. 

I agree that when it comes to analyzing these models and what they output, the real analysis and deep reflection have to start with us - the Homo sapiens sapiens. After all, we’re the ones who design, train, and interpret these systems. The challenge with using one model to analyze another is that it can create layers of abstraction, where each layer adds its own biases or assumptions. One model might interpret data patterns in a way that seems logical to its training, but completely misses the human or ethical nuance behind them. These layers can end up reinforcing blind spots instead of exposing them, which is why human insight and critical thinking are still essential.

Question to the Speakers
---
With everything coming out at a faster and faster rate, here is my question:
Where do we see the most need for red-teaming today?

Conclusion
---
Red-teaming is vital for the safety and quality of ai. When it is overlooked, the models inevitably lead to consequneces that could otherwise have been thwarted. As AI continues to grow faster than our ability to fully understand it, we need more people willing to slow down, test, and challenge what these systems say. It challenges my original views that AI is naturally unbiased when trained on the right datasets, but with the act of machine learning to train the models, these faults can come out. There are humans routinely checking the outputs of these models and ensuring their over saftey and accuracy.


Quote of the Blog
---
“The universe doesn't allow perfection.” - Stephen Hawking