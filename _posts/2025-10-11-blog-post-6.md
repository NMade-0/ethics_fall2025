---
title: 'The Reality Of Us'
date: 2025-10-11
permalink: /posts/2025/10/blog-post-6/
tags:
  - case study
  - AI
  - companion
  - ethics
  - Addiction
---

Overview
---
This blog post is on the article ("Addictive Intelligence")[https://mit-serc.pubpub.org/pub/iopjyxcx/release/2?readingCollection=132bb7af] and my take on the five discussion questions posted at the end of the article. I apologize for all the spelling mistakes you are about to encouter...

Summary
---
The article covers the rise of artificial intelligence, specifically focused on the rise of AI companions. Which, if you as the reader is unfamiliar, are large language models (think ChatGPT) that are trained and personified to provide real-time conversations on various subjects to the user. While the computational genius behind the models is remarkable and the fact that this was something of science fiction decade ago, the fact in the matter, is that these AI-companions are still just models. The personifictaion of these models leads to the ignorance, either blissed or from naivety, of their binary bits. The consequences from this misinterpetaion, and from the fact that no model is 100% hullucination-free, has led to fatal outcomes. Click on article title to check out the article or listen to the audio version provided.


Questions
---
What are the ethical problems to be creating AI companions that are easily accessible to people who may be unfamiliar with what is just ones and zeros vs sentient?--Like my granmamamama?????



#### 1. In the Sewell Setzer case, the AI’s response to suicidal ideation shifted from concern to potentially encouraging harmful behavior. How can companies design AI companions to be emotionally engaging while preventing harmful psychological dependencies? Discuss both technical safeguards (e.g., algorithmic oversight, intervention protocols) and ethical guidelines (e.g., duty of care, transparency).

The first safeguard here is the prevention altogether, which starts at the algorithic design and the data the models are trained on. Obviously with the astronomical size of the corpus (stored knowledge of training data), it is very challenging, and almost impossible, to prevent every harmful, psychological output. I do beleive, however, that the models reach a certain level of efficiency to where companies designing these models say "good enough" and release them out to the public. This comment, "good enough," leads to saving a lot of resources and time with building an AI companion -- leading to the goal of making income for the company. This apprach works until the "good enough" is no longer "enough" and someone inevitably falls victim to a model's faults. 

The second safeguard is the human intervention. If we can train models to have frightenly-real conversations in real-time, we should be able to train the model in on detecting when human intervention is needed. I believe this detection is often overlooked since it isn't resource-cheap to continue training a model.

#### 2. How does addiction to AI companions compare with other forms of technology addiction, such as social media or gaming? What unique features make AI companions potentially more addictive? Support your analysis with examples from the case study and other relevant cases.

I believe the "rules" for what an official addiction is to be very idiotic, and that anything that is compulsively appealing can cause, and be classified as, an addiction. To put it lightly, the fact that both AI companions and social media are designed to keep hold of the users for as long as possible, really piss me off. The amount of hours I have lost to social media is equivelant to watching the 140-minute-long Forest Gump film every night. The only difference is that Forest tells a good story of perseverance, while reels tell me nothing of the matter. 

The thing that makes AI really addictive is that, while they are very complex in how they are designed, they are very one-dimensional in giving you responses you want to hear. Like the submissive friend who never challenges your thinking -- even if that thining is stupid. It is like me when I daydream -- I always win.

#### 3. An elderly person finds genuine comfort in an AI companion, alleviating their loneliness, but their family worries this relationship is replacing real human connections. How should we evaluate the benefits versus risks in such cases? What ethical guidelines or intervention strategies might help determine when AI companionship crosses from beneficial to harmful?

My interpretation on this won't be the best since I am still young and will not be able to understand this at a personal level. 

I know the older one gets, at a certain point, people start slowing down. By that, I mean a person starts to need more and more accommadations to get around and the places and things one is able to do with others starts to become more limited. For elders finding peace at an AI friend can be concerning indeed, but we need to stop and ask ourselves why and how they are using AI to socialize in the first place. Are they able to get out much and meet people? Do they find the AI companion as a fun pass-time activity, or as a need and desire to connect? The risks here, are if the person in using the AI as a reason to connect more, which can be a problem if they choose to hide away more instead of going out and socializing which can feed back into the lonliness problem like a loop. Having a conversation with one who uses these AI companions or about whether they do use one in the frist place, can be the best first step to figuring out what is going on with someone using AI and hiding away. 

#### 4. Current business models incentivize AI companies to maximize user engagement. What alternative economic models could promote healthier AI interactions while maintaining commercial viability? Consider both market-driven solutions (e.g., subscription-based models, ethical AI certifications) and regulatory approaches (e.g., user well-being metrics, engagement caps).

I remember when a social media site, I believe it was Youtube, would show me a modal view saying that I should "take a break." As someone who is aware the more engagement means more income to the company, I have huge respect to those bold enough to do the ethical thing and take those binge watchers' health into account -- and not just at the "good enough" level.

This modal view solution should be a requirement. If I'm scrolling on Snapchat shorts and two hours have gone by, I need a wake-up call because I am no longer aware of what I am doing.


#### 5. If you were developing regulations for AI companions, how would you address age restrictions, usage limits, and safety monitoring while respecting user privacy and autonomy? Provide specific examples of how your proposed regulations could have helped prevent incidents like the Sewell Setzer case.

In a perfect world, under 16 is too young to be ruining your life doomscrolling on social media, especially if it's one of those sexually-role-playing models. In the article, it was mentioned that the "analysis of a million ChatGPT interaction logs reveals that the second most popular use of AI is sexual role-playing." TO address the issue, some form of age-verification should be required to AI models, but in ways that are repecting the users' privacy. 

As for how to accomplish the age restricitons, I can't think of any good ideas. I got...not-so-good ideas:
- Take an awareness quiz, if the user is unable to understand that the AI companion is not real, no access
- Ask for age with an eerie disclaimer about the risks of being too young using an AI companion
- If they don't know the seven continents of the world, they can't enter


Quote
---
The question is not whether intelligent machines can have any emotions, but whether machines can be intelligent without any emotions. – Marvin Minsky