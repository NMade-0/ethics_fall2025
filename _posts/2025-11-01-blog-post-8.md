---
title: 'Bittersweet AI'
date: 2025-10-07
permalink: /posts/2025/11/blog-post-8/
tags:
  - case study
  - bias
  - ethic
---

Summary
---
This article covers the harms that occur at different stages of the ML life cycle. From data collection to deployment, inevitably leading to harmful impacts to the population. It looks good from afar as they are getting created and what they are being claimed to solve. But in the end, they end up being overall violent. You can find the areticle [here](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2)

So What's Going On? 
---
The article covers seven sources of harm that that leads to bias in models. Let's go over them, because I know how much you are itching for seven explanations of bias:
- Historical Bias arises even if the data is accurate, becuase they reflect past societal inequities.
- Representations Bias is when the developement sample fials to represent the use population.
- Measurement Bias occurs when labels or features poorly capture what they intend to measure or differe by group.
- Aggregation Bias is the "one-model-fits-all" idea where applying one model to all groups even though, obviously, relationships differ between subgroups. (Ps. I learned the word aggregate last summer. I a smart one!)
- Learning Bias is when the decisions made to train the models actually amplify the inequities.
- Evaluation Bias is where the benchmarks and metrics don't reflect diverse populations.
- Finally and not least: Deployment bias is when real-world use differes form the model's intended design or context.

Phew, sorry about that. Not really.

How do to solve all this fricken fracken bias!? Well, for some models, it's the lack of representation and can be easily reduced with more data (Like an analogy made in the article to "help detect whether someone is having a heart attack"). For other models, it is a lot more challenging of a solution (As in the analogy made right after of resume-scanning for "predicitng the suitability of a candidate"). There is not "one-size-fits-all" solution. Each case and model need to be deconstructed and questioned, on all seven levels of bias. Then act accordingly.

Discussion Questions
---

####Can you come up with an aidditional example of each source of harm?

Why, of course I can! 
- Historical Bias: A hiring model facors men because pastpromotions mostly went to men,
- Representations Bias: A facial-recognition system fails on darke-skinned faces due to limited training diversity.
- Measurement Bias: A school predicts success form attendance, disadvantaging student with health issues.
- Aggregation Bias: A single health model misjudges systoms that differe between men and women.
- Learning Bias: A fraud detector trained for accuracy ignores rare but critical fraud cases.
- Evaluation Bias: A self-driving car tested only in daylight fails at night.
- Deployment Bias: A risk-score meant to flag follow-ups is used instead to deny medical treatment.



####How might these sources of harm manifest in a project you are currently working on, or have worked on in the past? 

I have done Apple app development in the past, and one area I’ve explored is using AI to help design or adapt the user interface based on user behavior. While this can make an app feel more personalized, it can also introduce bias in subtle ways. For example, if the AI learns layout or color preferences from a small group of early users, it might overfit to their habits and fail to reflect the needs of a broader, more diverse audience. It could start prioritizing designs that appeal to certain demographics while unintentionally making the interface less intuitive for others. If Doe John the butter chef wants to use an app, but is green-blue color blind, and the UI is trained on the idea of "Modern UI equals Blues and Greens," then it sucks to be Doe John since the ai model was biased.

####Draft a checklist that people starting new projects could use to try to anticipate or prevent different types of bias.

# Big Beautiful Bias Prevention Checklist

## Planning
- [ ] Define project goals and affected users.  
- [ ] Identify assumptions about data and behavior.  

## Data
- [ ] Use diverse and up-to-date data.  
- [ ] Avoid weak proxies for real outcomes.  

## Design
- [ ] Test UI with varied users and devices.  
- [ ] Follow accessibility standards.  

## AI Models
- [ ] Check fairness across groups.  
- [ ] Document model choices and objectives.  

## Testing
- [ ] Evaluate with multiple metrics and conditions.  
- [ ] Compare results by user subgroup.  

## Deployment
- [ ] Monitor real-world use and feedback.  
- [ ] Fix or disable harmful behaviors quickly.




Question to the Speakers
---
How can I manage these models' biases when I am in a setting or location that has adapted the use of them, like in a hospital risk detection or a speeding ticket fee?

Conclusion
---
Bias in ML is not confined to data (that's a fancy way of saying, it is not just the data they're trained on). It is distributed across every stage of the pipeline. Recognizing specific sources of harm enables more precise, context-aware solutions. Ethical ML design demands understanding the data’s origins, the model’s role, and the human systems it enters.

Quote of the Blog
---
"Make a habit of two things: to help; or at least to do no harm." - Hippocrates