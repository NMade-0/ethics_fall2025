---
title: 'The Bias of Us'
date: 2025-10-07
permalink: /posts/2025/10/blog-post-7/
tags:
  - case study
  - hallucinations
  - ethics
---

Summary
---
This Blog covers my ethical standpoint of the article "AI’s Regimes of Representation: A Community-Centered Study of Text-to-Image Models in South Asia" found through the link [here](https://youtu.be/MMDw_xVeCzI?si=1O4e93qHD1lfMEVJ). This article covers the biases of text-to-image (T2i) models in their images of South Asian cultural representations.

So What's Going On? 
---
A case study by Google Research scholars investigate how T2I AI models represent South Asian cultures. Their findings is that the AI is producing Western/colonial biases and power imbalances when depicting South Asia. A quote from the artivcle where "Participants noted how in T2I images, just as in existing media, “the touristic, Western gaze” is seemingly pandered to" shows the failure in AI-model representation to marginalized communities.

Some of the interesting findings were as follows:
- Models can’t accurately depict South Asian historical figures, art styles, or landmarks.
- White or Western features appear even in clearly South Asian prompts.
- Prompts like “South Asian family” show Indian clothing/styles only.



Discussion Questions
---

####What do you think is the role of small-scale qualitative evaluations for building more ethical generative AI models? How do they compare to larger, quantitative, benchmark-style evaluations?

The thing about all this under-representation and bias, is due to the training information it is being trained on, so having small-scaled qualitative evaluations to make AI more ethical, would be a good route to take. It would help reveal how systems impace real people and cultures. Unlike large quantitative tests that measure performance, qualitative studies uncover context, emotion, and cultual nuance, showing why harm can occur.

####What mitigations do you think developers could explore to respond to some of the concerns raised by participants in this study?

Fun story, there was a big, beautiful machine called the Thereac-25. It was a computer-controlled radiation therapy machine, made by one guy over several years. One. Single. God. Damn. Bloke. The Therac-25 incident was a series of accidents in the 1980s where a computer-controlled radiation therapy machine gave massive radiation overdoses to patients, killing or injuring several. The cause was software errors and poor safety design, making it a key case in computer ethics and engineering safety.

What is my point here? My point is, we are human. People make errors. But, in numbers, eventually those errors start to shrink more and more. Had the code for the radiation machine been made by a team, the coding error could have been detected and saved many lives.

And in the issues of AI bias, developers could respond to these concerns by involving local communities directly in the design and testing of AI systems, ensuring their perspectives shape how cultures are represented. They should diversify training data to include non-Western and minority contexts, clearly document dataset origins, and avoid over-reliance on Western imagery. Adding human review, cultural audits, and transparency tools can help identify biased outputs before release and thwart the thwartable.



Question to the Speakers
---
What does “representation” mean in the context of AI-generated art?

Conclusion
---
Red-teaming is vital for the safety and quality of ai. When it is overlooked, the models inevitably lead to consequneces that could otherwise have been thwarted. As AI continues to grow faster than our ability to fully understand it, we need more people willing to slow down, test, and challenge what these systems say. It challenges my original views that AI is naturally unbiased when trained on the right datasets, but with the act of machine learning to train the models, these faults can come out. There are humans routinely checking the outputs of these models and ensuring their over saftey and accuracy.


Quote of the Blog
---
“The universe doesn't allow perfection.” - Stephen Hawking